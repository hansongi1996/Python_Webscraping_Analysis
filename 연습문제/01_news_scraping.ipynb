{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8beef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: requests\n",
      "Version: 2.32.3\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: C:\\Users\\user\\anaconda3\\Lib\\site-packages\n",
      "Requires: certifi, charset-normalizer, idna, urllib3\n",
      "Required-by: aext-assistant-server, anaconda-auth, anaconda-catalogs, anaconda-client, anaconda-project, conda, conda-build, conda-repo-cli, conda_package_streaming, cookiecutter, datashader, jupyterlab_server, panel, requests-file, requests-toolbelt, Sphinx, streamlit, tldextract\n"
     ]
    }
   ],
   "source": [
    "# requests 라이브러리 설치여부 확인\n",
    "!pip show requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7941aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: beautifulsoup4\n",
      "Version: 4.12.3\n",
      "Summary: Screen-scraping library\n",
      "Home-page: https://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "Author: \n",
      "Author-email: Leonard Richardson <leonardr@segfault.org>\n",
      "License: MIT License\n",
      "Location: C:\\Users\\user\\anaconda3\\Lib\\site-packages\n",
      "Requires: soupsieve\n",
      "Required-by: conda-build, nbconvert\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup4 라이브러리 설치여부 확인\n",
    "!pip show beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b22050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqeusts, bs4 import\n",
    "import requests\n",
    "# BeautifulSoup import\n",
    "import bs4\n",
    "# beautifulsoup4 클래스\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b2ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests 버전 = 2.32.3\n",
      "bs4버전 = 4.12.3\n"
     ]
    }
   ],
   "source": [
    "# requests, bs4 버전 확인하기\n",
    "print(f'requests 버전 = {requests.__version__}')\n",
    "print(f'bs4버전 = {bs4.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba80bf",
   "metadata": {},
   "source": [
    "### 질문1. 뉴스 제목 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3d5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.daum.net/breakingnews/economic\n"
     ]
    }
   ],
   "source": [
    "# 다음 경제 뉴스 URL\n",
    "req_param = {\n",
    "    'cat': 'economic'  # 경제 'economic'\n",
    "}\n",
    "\n",
    "url = 'https://news.daum.net/breakingnews/{cat}'.format(**req_param)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de121327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.daum.net/economy\n",
      "<class 'requests.models.Response'>\n",
      "200\n",
      "<class 'bs4.element.ResultSet'> 9\n",
      "https://v.daum.net/v/20250723203204354\n",
      "'법인세 부담' 언론 보도가 놓치는 것, 명목보다 실질을 봐야 한다\n",
      "https://v.daum.net/v/20250723200257745\n",
      "스테이블 : USDT의, USDT에 의한, USDT를 위한 디지털 국가\n",
      "https://v.daum.net/v/20250723193636084\n",
      "쌀·소고기 대신 '연료작물' 통할까…\"조선·제조업 내세워 美 설득해야\"\n",
      "https://v.daum.net/v/20250723192143583\n",
      "‘759조 투자·쌀 개방’ 일본 통 큰 양보…정부 대미 협상안 긴급 재조정\n",
      "https://v.daum.net/v/20250723191424311\n",
      "비트코인 다 오른다는데… 20만 달러 갈까 고점서 빠질까\n",
      "https://v.daum.net/v/20250723190948187\n",
      "“배당·장기투자 문화 정착시키고 기관투자자 적극 역할을” [2025 세계증권포럼]\n",
      "https://v.daum.net/v/20250723190857159\n",
      "“해외투자·서비스 수출… 韓·글로벌 경제 ‘질적 연결’ 늘려야” [2025 세계증권포럼]\n",
      "https://v.daum.net/v/20250723190302963\n",
      "주4.5일제 찬반 ‘팽팽’…금융권 선도 vs 부작용 우려\n",
      "https://v.daum.net/v/20250723185405689\n",
      "與안도걸 \"스테이블코인 법안 내주 발의\"…통화·외환 파장도 예의주시\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 다음 경제 뉴스 URL\n",
    "url = 'https://news.daum.net/economy'\n",
    "\n",
    "print(url)\n",
    "\n",
    "# 요청 헤더\n",
    "req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "res = requests.get(url, headers=req_header)\n",
    "print(type(res))\n",
    "print(res.status_code)\n",
    "\n",
    "# 정상 응답 여부 확인\n",
    "if res.ok:\n",
    "    # 응답 데이터 인코딩\n",
    "    res.encoding = 'utf-8' \n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # 기사 제목과 링크 추출\n",
    "    li_tag_list = soup.select('ul.list_newsheadline2 li')\n",
    "\n",
    "    print(type(li_tag_list), len(li_tag_list))\n",
    "\n",
    "    for li_tag in li_tag_list:  #li_tag => <li>     \n",
    "        a_tag = li_tag.find('a') #a_tag <a href=\"\">\n",
    "        print(a_tag['href'])\n",
    "        \n",
    "        #strong_tag = li_tag.select('div.cont_thumb strong.tit_txt')[0] #[<strong>]\n",
    "        strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "        title = strong_tag.text.strip()\n",
    "        print(title)\n",
    "\n",
    "else:\n",
    "    print(f'에러코드 = {res.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d1aa2",
   "metadata": {},
   "source": [
    "### 질문2. url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c40d70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# '사회': 'society'\n",
    "def print_news(section_name):    \n",
    "    section = section_dict[section_name]\n",
    "    # 요청 Parameter\n",
    "    req_param = {\n",
    "        'section': section\n",
    "    }\n",
    "    #url = 'https://news.daum.net/{section}'.format(**req_param)\n",
    "    url = f'https://news.daum.net/{section}'\n",
    "    \n",
    "    print(f'======> {url} {section_name} 뉴스 <======')\n",
    "    \n",
    "    # 요청 헤더 설정 : 브라우저 정보\n",
    "    req_header = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, headers=req_header)   \n",
    "\n",
    "    if res.ok:\n",
    "        res.encoding = 'utf-8' \n",
    "        html = res.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        li_tag_list = list()\n",
    "        # 기사 제목과 링크 추출\n",
    "        li_tag_list = soup.select('ul.list_newsheadline2 li')\n",
    "\n",
    "        #print(type(li_tag_list), len(li_tag_list))\n",
    "\n",
    "        for li_tag in li_tag_list:        \n",
    "            a_tag = li_tag.find('a')\n",
    "            print(a_tag['href'])\n",
    "            \n",
    "            strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "            title = strong_tag.text.strip()\n",
    "            print(title)\n",
    "\n",
    "    else:\n",
    "        print(f'에러코드 = {res.status_code}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
